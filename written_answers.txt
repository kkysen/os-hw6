Answers for parts  1 - 5
Enter your answers in the designated location. Do NOT remove lines that start
with '=' signs. Removing these lines will break our grading scrips and will
result in 0 points. Also, keep lines to a max of 80 chars long (you do not
need to worry if the top command is longer than 80 chars). Also, please limit
your answers to about 40 words.

================================== P1Q1 start ==================================
Describe how you created the 70%/30% split. 
    - Include the command lines you executed
    - Indicate if you needed root privileges for any of those commands
    - Include the top output

We ran the command
```shell
for i in {1..10}; do
    [[ $i -le 5 ]] && nice=-7 || nice=-3
    taskset 1 nice $nice yes > /dev/null &
done
```
to create the 70%/30% split.

Root privileges were not necessary for any of these commands.

```htop
  1  [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]   5  [                                                                               0.0%]
  2  [                                                                               0.0%]   6  [                                                                               0.0%]
  3  [                                                                               0.0%]   7  [                                                                               0.0%]
  4  [                                                                               0.0%]   8  [|                                                                              0.7%]
  Mem[|||||||||                                                                618M/12.3G]   Tasks: 36, 53 thr; 8 running
  Swp[                                                                           0K/4.00G]   Load average: 9.99 7.53 4.06
                                                                                             Uptime: 22:13:01

  PID USER      PRI  NI  VIRT   RES   SHR S CPU% MEM%   TIME+  Command
 2330 kkysen     23   3  7236   588   520 R 14.6  0.0  0:57.21 yes
 2328 kkysen     23   3  7236   588   520 R 14.0  0.0  0:57.21 yes
 2332 kkysen     23   3  7236   524   456 R 14.0  0.0  0:57.21 yes
 2326 kkysen     23   3  7236   596   524 R 14.0  0.0  0:57.21 yes
 2324 kkysen     23   3  7236   596   524 R 14.0  0.0  0:57.21 yes
 2316 kkysen     27   7  7236   592   520 R  6.0  0.0  0:23.38 yes
 2320 kkysen     27   7  7236   592   520 R  6.0  0.0  0:23.38 yes
 2318 kkysen     27   7  7236   592   520 R  6.0  0.0  0:23.38 yes
 2314 kkysen     27   7  7236   524   456 R  6.0  0.0  0:23.38 yes
 2322 kkysen     27   7  7236   588   520 R  5.3  0.0  0:23.38 yes
```

=================================== P1Q1 end ===================================

================================== P1Q2 start ==================================
Describe how you created a real-time priority task.
    - Include the command lines you executed
    - Indicate if you needed root privileges for any of those commands
    - Include the top output

After running the command previously used to create the 70%/30% split,
we ran `taskset 1 sudo chrt --rr 99 yes > /dev/null`.

`chrt` here, specifically the `sched_setscheduler` syscall,
needed root privileges to run.

```htop
  1  [|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]   5  [                                                                               0.0%]
  2  [                                                                               0.0%]   6  [|                                                                              0.7%]
  3  [                                                                               0.0%]   7  [|                                                                              0.7%]
  4  [                                                                               0.0%]   8  [                                                                               0.0%]
  Mem[|||||                                                                    127M/12.3G]   Tasks: 36, 2 thr; 8 running
  Swp[                                                                           0K/4.00G]   Load average: 4.92 3.09 2.08
                                                                                             Uptime: 00:30:58

  PID USER      PRI  NI  VIRT   RES   SHR S CPU% MEM%   TIME+  Command
 3149 root       RT   0  7232   524   456 R 97.4  0.0  0:19.58 yes
 3106 kkysen     23   3  7236   588   520 R  0.7  0.0  0:02.36 yes
 3103 kkysen     23   3  7236   588   516 R  0.7  0.0  0:02.35 yes
 3105 kkysen     23   3  7236   528   456 R  0.7  0.0  0:02.35 yes
 3098 kkysen     27   7  7236   592   520 R  0.7  0.0  0:00.96 yes
 3170 kkysen     20   0  8564  4072  3044 R  0.7  0.0  0:00.03 htop
 3104 kkysen     23   3  7236   524   456 R  0.0  0.0  0:02.35 yes
 3102 kkysen     23   3  7236   588   516 R  0.0  0.0  0:02.35 yes
 3101 kkysen     27   7  7236   524   456 R  0.0  0.0  0:00.96 yes
 3100 kkysen     27   7  7236   524   456 R  0.0  0.0  0:00.96 yes
 3097 kkysen     27   7  7236   592   520 R  0.0  0.0  0:00.95 yes
```

=================================== P1Q2 end ===================================



================================== P2Q1 start ==================================
The output of diff or diffconfig when comparing the config files for your 
mainline fallback kernel and your MuQSS kernel

-ANDROID_BINDERFS n
-ANDROID_BINDER_DEVICES "binder"
-ANDROID_BINDER_IPC_SELFTEST n
-BPF_UNPRIV_DEFAULT_OFF y
-CFS_BANDWIDTH y
-CGROUP_CPUACCT y
-FAIR_GROUP_SCHED y
-INTEL_IOMMU_DEFAULT_OFF n
-INTEL_IOMMU_DEFAULT_ON_INTGPU_OFF y
-LOCK_DOWN_IN_EFI_SECURE_BOOT y
-MTD_PSTORE n
-NUMA_BALANCING y
-NUMA_BALANCING_DEFAULT_ENABLED y
-PSTORE_BLK m
-PSTORE_BLK_BLKDEV ""
-PSTORE_BLK_KMSG_SIZE 64
-PSTORE_BLK_MAX_REASON 2
-PSTORE_ZONE m
-RESET_BRCMSTB_RESCAL n
-RT_GROUP_SCHED n
-SCHED_AUTOGROUP y
-SECURITY_PERF_EVENTS_RESTRICT y
-X86_X32_DISABLED y
-XILINX_ZYNQMP_DPDMA n
 ANDROID_BINDER_IPC m -> n
 ASHMEM m -> n
 BLK_DEV_LOOP m -> y
 CC_VERSION_TEXT "gcc-10 (Debian 10.2.1-6) 10.2.1 20210110" -> "gcc (Debian 10.2.1-6) 10.2.1 20210110"
 LOCALVERSION "" -> "-ajr2252-muqss"
 SYSTEM_TRUSTED_KEYS "debian/certs/debian-uefi-certs.pem" -> ""
 USB_COMMON m -> y
+BATTERY_RT5033 n
+FB_NVIDIA n
+FB_RIVA n
+NTFS_FS n
+RQ_ALL n
+RQ_MC y
+RQ_MC_LLC n
+RQ_NONE n
+RQ_SMP n
+RQ_SMT n
+SCHED_MUQSS y
+SHARERQ 2
+SMT_NICE y
+SYSTEM_REVOCATION_LIST n
+VIDEO_VS6624 n

=================================== P2Q1 end ===================================

================================== P2Q2 start ==================================
Indicate you successfully patched, built, and booted into your MuQSS-enabled 
Linux kernel.

Successfully patched, built, and booted into the MuQSS-enabled Linux kernel.
The following is the output of `sudo dmesg | grep "MuQSS"`:

[    0.159554] MuQSS possible/present/online CPUs: 128/2/2
[    0.159555] MuQSS locality CPU 0 to 0: 0
[    0.159555] MuQSS locality CPU 0 to 1: 4
[    0.159555] MuQSS locality CPU 1 to 0: 4
[    0.159556] MuQSS locality CPU 1 to 1: 0
[    0.159564] MuQSS CPU 0 llc 0 RQ order 0 RQ 0 llc 0
[    0.159564] MuQSS CPU 0 llc 0 RQ order 1 RQ 1 llc 2
[    0.159565] MuQSS CPU 1 llc 2 RQ order 0 RQ 1 llc 2
[    0.159565] MuQSS CPU 1 llc 2 RQ order 1 RQ 0 llc 0
[    0.159566] MuQSS CPU 0 llc 0 CPU order 0 RQ 0 llc 0
[    0.159566] MuQSS CPU 0 llc 0 CPU order 1 RQ 1 llc 2
[    0.159567] MuQSS CPU 1 llc 2 CPU order 0 RQ 1 llc 2
[    0.159567] MuQSS CPU 1 llc 2 CPU order 1 RQ 0 llc 0
[    0.159567] MuQSS runqueue share type MC total runqueues: 2
[    9.019688] MuQSS CPU scheduler v0.205 by Con Kolivas.

=================================== P2Q2 end ===================================



================================== P3Q1 start ==================================
Describe how you created the 70%/30% split. 
    - Include the command lines you executed
    - Indicate if you needed root privileges for any of those commands
    - How were the results different from P1Q1, if at all.

We ran the same command as in P1Q1 to create the 70%/30% split:
```shell
for i in {1..10}; do
    [[ $i -le 5 ]] && nice=-7 || nice=-3
    taskset 1 nice $nice yes > /dev/null &
done
```

Like with CFS, no root privileges were needed.

Compared to P1Q1, the split wasn't exactly 70%/30%, it was more like 60%/40%,
even though the same -7 and -3 nice values were used.

=================================== P3Q1 end ===================================

================================== P3Q2 start ==================================
Describe how you created a real-time priority task. 
    - Include the command lines you executed
    - Indicate if you needed root privileges for any of those commands
    - How were the results different from P1Q2, if at all.

After running the command previously used to create the 70%/30% split,
we ran `taskset 1 chrt --rr 99 yes > /dev/null`.

Unlike with CFS, no root privileges were stricly needed to run `chrt` this time.

Compared to P1Q2, the real-time priority task does not achieve
anywhere near 100% CPU.  In P1Q2, it got close at high 90s%,
but here it fluctuates between 50% and 80%.

=================================== P3Q2 end ===================================

================================== P3Q3 start ==================================
MuQSS features unprivileged real-time tasks. Perform the previous task with and
without root privileges, and describe the differences. 

When running the previous command with `sudo chrt` instead of `chrt`,
the real-time priority task was able to achieve close to 100% CPU like in P1Q2.

=================================== P3Q3 end ===================================



================================== P4Q1 start ==================================
Verify Con Kolivas' claim by timing the kernel build-time in both your fallback
and your MuQSS-patched kernels.

Results:
Fallback kernel: 134.00 user,    34.20 system,   0 status
MuQSS-patch kernel: 164.19 user,    25.39 system,   0 status

Our findings indicate that they have similar build times,
although the fallback kernel appears to win against the MuQSS-patch in
user space time, for time in kernel space the patch beats the original
fallback kernel, which supports Con Kolivas' claim.

=================================== P4Q1 end ===================================

================================== P4Q2 start ==================================
Design an experiment that you think will highlight MuQSS’s strength. Perform 
the experiment and report your findings.

Our experiment was to play a playlist of Youtube videos at high-quality
on the VM while the kernel built.
Our command was
```shell
sudo /usr/bin/time -f "\t%U user,\t%S system,\t%x status" make -j2
```

Time for the fallback kernel: 145.64 user,    41.50 system,   0 status
Time for the MuQSS-patched kernel: 139.86 user,    25.29 system,   0 status

The experiment validated Con Kolivas' claim, more than P4Q1.
The MuQss version was lower (although similar) in user mode
and time in kernel mode was cut down by almost half (41.50 to 25.29)
in the MuQSS version.

=================================== P4Q2 end ===================================



================================== P5Q1 start ==================================
Briefly describe the advantages and disadvantages of a larger HZ.

    Advantages
A larger HZ means timer interrupts are more frequent,
so work is more fine-grained and accuracy and resolution for timers
and their events is greater (though worse than high resolution timers).

    Disadvantages
More time is spent doing actual timer interrupts,
which may also thrash the caches.

=================================== P5Q1 end ===================================

================================== P5Q2 start ==================================
What is the HZ currently configured for your running Linux system?

(kernel) HZ = 250 = `grep 'CONFIG_HZ=' /boot/config-$(uname -r)`.
USER_HZ = 100 = `getconf CLK_TCK`

=================================== P5Q2 end ===================================

================================== P5Q3 start ==================================
What are jiffies? Explain the relationship between jiffies, HZ, and time.

Jiffies is the number of ticks/timer interrupts since boot-time.
HZ is the number of interrupts/second, so HZ jiffies happen every second.

time (in seconds) = jiffies/HZ

=================================== P5Q3 end ===================================

================================== P5Q4 start ==================================
Find the current value of jiffies in your system.
    - In minutes, how much time does this jiffies value represent?
    - Does it match the uptime reported by the uptime command? (Hint: it 
      doesn’t.) Please give the formula to convert jiffies to the current 
      (real) uptime, in minutes.
    - Why does this large difference exist? (Hint: in 32-bit Linux systems,
      jiffies is a 32-bit value.)

We got:
    initial_jiffies = 4294892296
        (from `#define INITIAL_JIFFIES` in `include/linux/jiffies.h`)
    jiffies = 4295219340 (from `/proc/timer_list`)
    jiffy_time = (jiffies - initial_jiffies) / HZ / 60 minutes
               = (4295219340 - 4294892296) / 250 / 60 minutes
               = 21.80 minutes
    uptime = 22.12 minutes
Without subtracting the initial jiffies, we get:
    jiffy time = jiffies / HZ / 60 = 4295219340 / 250 / 60 = 286347.96 minutes

The large difference exists because jiffies starts as a high 32-bit integer
to detect wrapping overflow, but on a 64-bit CPU this doesn't happen
for a long time.  Once the initial jiffies are subtracted,
the jiffies are very close to the uptime.

=================================== P5Q4 end ===================================

================================== P5Q5 start ==================================
What are Niffies? How do they differ from Jiffies?

Niffies are similar to jiffies except they’re calculated in nanoseconds
rather than seconds, and thus must be calculated from high resolution
TSC timers for each runqueue. Since they’re per-runqueue, they are
periodically synchronized between CPUs when runqueues are concurrently locked.

=================================== P5Q5 end ===================================
